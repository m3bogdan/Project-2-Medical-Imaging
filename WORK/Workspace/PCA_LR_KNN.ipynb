{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from skimage import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, classification_report, f1_score, roc_auc_score\n",
    "import pickle\n",
    "import Functions2\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\serru\\\\OneDrive\\\\Documents\\\\Project2\\\\Project-2-Medical-Imaging\\\\data\\\\ColorMask\\\\Training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load the data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m data_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mUsers\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mserru\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOneDrive\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mDocuments\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProject2\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProject-2-Medical-Imaging\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mColorMask\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(data_path)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Preprocess the diagnostic column\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mdiagnostic\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mdiagnostic\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap({\u001b[39m'\u001b[39m\u001b[39mBCC\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMEL\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSCC\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mACK\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNEV\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSEK\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:586\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    571\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    572\u001b[0m     dialect,\n\u001b[0;32m    573\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    583\u001b[0m )\n\u001b[0;32m    584\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 586\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:482\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    479\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    481\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 482\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    484\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    485\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:811\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwds:\n\u001b[0;32m    809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m--> 811\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1040\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1036\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1037\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown engine: \u001b[39m\u001b[39m{\u001b[39;00mengine\u001b[39m}\u001b[39;00m\u001b[39m (valid options are \u001b[39m\u001b[39m{\u001b[39;00mmapping\u001b[39m.\u001b[39mkeys()\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1038\u001b[0m     )\n\u001b[0;32m   1039\u001b[0m \u001b[39m# error: Too many arguments for \"ParserBase\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[39mreturn\u001b[39;00m mapping[engine](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:51\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     48\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39musecols\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39musecols\n\u001b[0;32m     50\u001b[0m \u001b[39m# open handles\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open_handles(src, kwds)\n\u001b[0;32m     52\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m# Have to pass int, would break tests using TextReader directly otherwise :(\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:222\u001b[0m, in \u001b[0;36mParserBase._open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_handles\u001b[39m(\u001b[39mself\u001b[39m, src: FilePathOrBuffer, kwds: \u001b[39mdict\u001b[39m[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    Let the readers open IOHandles after they are done with their potential raises.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m    223\u001b[0m         src,\n\u001b[0;32m    224\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    225\u001b[0m         encoding\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    226\u001b[0m         compression\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    227\u001b[0m         memory_map\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m    228\u001b[0m         storage_options\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    229\u001b[0m         errors\u001b[39m=\u001b[39;49mkwds\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    230\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\serru\\.conda\\envs\\New\\lib\\site-packages\\pandas\\io\\common.py:702\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    698\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    699\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    701\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 702\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    703\u001b[0m             handle,\n\u001b[0;32m    704\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    705\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    706\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    707\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    708\u001b[0m         )\n\u001b[0;32m    709\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    711\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\serru\\\\OneDrive\\\\Documents\\\\Project2\\\\Project-2-Medical-Imaging\\\\data\\\\ColorMask\\\\Training'"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "data_path = r'C:\\Users\\serru\\OneDrive\\Documents\\Project2\\Project-2-Medical-Imaging\\data\\ColorMask\\Training'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Preprocess the diagnostic column\n",
    "df['diagnostic'] = df['diagnostic'].map({'BCC': 1, 'MEL': 1, 'SCC': 1, 'ACK': 0, 'NEV': 0, 'SEK': 0})\n",
    "\n",
    "# Define the function to extract features\n",
    "def extract_features(folder_path):\n",
    "    feature_1 = []\n",
    "    feature_2 = []\n",
    "    feature_3 = []\n",
    "    feature_4 = []\n",
    "    feature_5 = []\n",
    "    feature_6 = []\n",
    "    feature_7 = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            original = io.imread(image_path)\n",
    "\n",
    "            # Ignore the alpha channel (e.g. transparency)\n",
    "            if original.shape[-1] == 4:\n",
    "                original = original[..., :3]\n",
    "\n",
    "            feature_1.append(Functions2.measure_pigment_network(original))\n",
    "            feature_2.append(Functions2.measure_blue_veil(original))\n",
    "            feature_3.append(Functions2.measure_vascular(original))\n",
    "            feature_4.append(Functions2.measure_globules(original))\n",
    "            feature_5.append(Functions2.measure_streaks(original))\n",
    "            feature_6.append(Functions2.measure_irregular_pigmentation(original))\n",
    "            feature_7.append(Functions2.measure_regression(original))\n",
    "\n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7\n",
    "\n",
    "# Define the folder path for image processing\n",
    "folder_path_in = r'C:\\Users\\serru\\OneDrive\\Documents\\Project2\\Project-2-Medical-Imaging\\data\\full_data.csv'\n",
    "\n",
    "# Extract features from the images\n",
    "feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7 = extract_features(folder_path_in)\n",
    "\n",
    "# Create a DataFrame for the features\n",
    "features_df = pd.DataFrame()\n",
    "features_df[\"img_id\"] = [filename for filename in os.listdir(folder_path_in) if filename.endswith(('.jpg', '.png'))]\n",
    "features_df[\"1: pigment network\"] = feature_1\n",
    "features_df[\"2: Blue veil\"] = feature_2\n",
    "features_df[\"3: Vascular\"] = feature_3\n",
    "features_df[\"4: Globules\"] = feature_4\n",
    "features_df[\"5: Streaks\"] = feature_5\n",
    "features_df[\"6: Pigmentation\"] = feature_6\n",
    "features_df[\"7: Regression\"] = feature_7\n",
    "\n",
    "# Merge the features DataFrame with the diagnostic column from the original DataFrame\n",
    "df_merged2 = pd.merge(df[['img_id', 'diagnostic']], features_df, on='img_id', how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = r\"/Users/bogdancristianmihaila/Desktop/2nd Semester/Github/project2/Project-2-Medical-Imaging/data/helpme/newdata.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "new_df['img_id'] = df['image']\n",
    "new_df['diagnostic'] = df[['MEL', 'BCC', 'AKIEC']].any(axis=1).astype(int)\n",
    "df = new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract features\n",
    "def extract_features(folder_path):\n",
    "    feature_1 = []\n",
    "    feature_2 = []\n",
    "    feature_3 = []\n",
    "    feature_4 = []\n",
    "    feature_5 = []\n",
    "    feature_6 = []\n",
    "    feature_7 = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            original = io.imread(image_path)\n",
    "\n",
    "            # Ignore the alpha channel (e.g. transparency)\n",
    "            if original.shape[-1] == 4:\n",
    "                original = original[..., :3]\n",
    "\n",
    "            feature_1.append(Functions2.measure_pigment_network(original))\n",
    "            feature_2.append(Functions2.measure_blue_veil(original))\n",
    "            feature_3.append(Functions2.measure_vascular(original))\n",
    "            feature_4.append(Functions2.measure_globules(original))\n",
    "            feature_5.append(Functions2.measure_streaks(original))\n",
    "            feature_6.append(Functions2.measure_irregular_pigmentation(original))\n",
    "            feature_7.append(Functions2.measure_regression(original))\n",
    "\n",
    "    return feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path for image processing\n",
    "folder_path_in = r\"/Users/bogdancristianmihaila/Desktop/2nd Semester/Github/project2/Project-2-Medical-Imaging/data/both_resized\"\n",
    "\n",
    "# Extract features from the images\n",
    "feature_1, feature_2, feature_3, feature_4, feature_5, feature_6, feature_7 = extract_features(folder_path_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the features\n",
    "features_df = pd.DataFrame()\n",
    "features_df[\"img_id\"] = [filename for filename in os.listdir(folder_path_in) if filename.endswith(('.jpg', '.png'))]\n",
    "features_df[\"1: pigment network\"] = feature_1\n",
    "features_df[\"2: Blue veil\"] = feature_2\n",
    "features_df[\"3: Vascular\"] = feature_3\n",
    "features_df[\"4: Globules\"] = feature_4\n",
    "features_df[\"5: Streaks\"] = feature_5\n",
    "features_df[\"6: Pigmentation\"] = feature_6\n",
    "features_df[\"7: Regression\"] = feature_7\n",
    "\n",
    "# Merge the features DataFrame with the diagnostic column from the original DataFrame\n",
    "df_merged = pd.merge(df[['img_id', 'diagnostic']], features_df, on='img_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['img_id'] = features_df['img_id'].str.replace('.jpg', '')\n",
    "\n",
    "df_merged = pd.merge(df[['img_id', 'diagnostic']], features_df, on='img_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#add the 2 datasets one under the other\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_merged \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_merged, df_merged2], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m df_merged\n\u001b[0;32m      4\u001b[0m \u001b[39m#save the new dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "#add the 2 datasets one under the other\n",
    "df_merged = pd.concat([df_merged, df_merged2], ignore_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X = df_merged.drop(['img_id', 'diagnostic'], axis=1)\n",
    "Y = df_merged['diagnostic']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_merged' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_merged\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_merged' is not defined"
     ]
    }
   ],
   "source": [
    "df_merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifiers without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "F1 score: 0.8974358974358974\n",
      "Precision: 0.8641975308641975\n",
      "Recall/Sensitivity: 0.9333333333333333\n",
      "Specificity: 0.65625\n",
      "Confusion Matrix:\n",
      "[[21 11]\n",
      " [ 5 70]]\n",
      "\n",
      "Classifier: KNeighborsClassifier\n",
      "F1 score: 0.8227848101265823\n",
      "Precision: 0.7831325301204819\n",
      "Recall/Sensitivity: 0.8666666666666667\n",
      "Specificity: 0.4375\n",
      "Confusion Matrix:\n",
      "[[14 18]\n",
      " [10 65]]\n",
      "\n",
      "Classifier: DecisionTreeClassifier\n",
      "F1 score: 0.7272727272727272\n",
      "Precision: 0.7647058823529411\n",
      "Recall/Sensitivity: 0.6933333333333334\n",
      "Specificity: 0.5\n",
      "Confusion Matrix:\n",
      "[[16 16]\n",
      " [23 52]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "\n",
    "classifiers = [LR(), KNN(), DTC()]  # Replace with your trained classifiers\n",
    "\n",
    "# Perform cross-validation for each classifier\n",
    "for classifier in classifiers:\n",
    "    y_pred = cross_val_predict(classifier, X, Y, cv=5)  # Change cv value as per your requirement\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    f1 = f1_score(Y, y_pred)\n",
    "    precision = precision_score(Y, y_pred)\n",
    "    recall = recall_score(Y, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(Y, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f\"Classifier: {type(classifier).__name__}\")\n",
    "    print(f\"F1 score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall/Sensitivity: {recall}\")\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(Y, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training classifiers with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: LogisticRegression\n",
      "F1 score: 0.6072874493927125\n",
      "Precision: 0.635593220338983\n",
      "Recall/Sensitivity: 0.5813953488372093\n",
      "Specificity: 0.49411764705882355\n",
      "Confusion Matrix:\n",
      "[[42 43]\n",
      " [54 75]]\n",
      "\n",
      "Classifier: KNeighborsClassifier\n",
      "F1 score: 0.654275092936803\n",
      "Precision: 0.6285714285714286\n",
      "Recall/Sensitivity: 0.6821705426356589\n",
      "Specificity: 0.38823529411764707\n",
      "Confusion Matrix:\n",
      "[[33 52]\n",
      " [41 88]]\n",
      "\n",
      "Classifier: DecisionTreeClassifier\n",
      "F1 score: 0.6463878326996197\n",
      "Precision: 0.6343283582089553\n",
      "Recall/Sensitivity: 0.6589147286821705\n",
      "Specificity: 0.4235294117647059\n",
      "Confusion Matrix:\n",
      "[[36 49]\n",
      " [44 85]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import cross_val_predict\n",
    "# from sklearn.metrics import f1_score, confusion_matrix, precision_score, recall_score\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# classifiers = [LR(), KNN(), DTC()]  # Replace with your trained classifiers\n",
    "\n",
    "# # Perform PCA\n",
    "# pca = PCA(0.95)  # Adjust the number of components as per your requirement\n",
    "# X_pca = pca.fit_transform(X)\n",
    "\n",
    "# # Perform cross-validation for each classifier\n",
    "# for classifier in classifiers:\n",
    "#     y_pred = cross_val_predict(classifier, X_pca, Y, cv=5)  # Change cv value as per your requirement\n",
    "    \n",
    "#     # Calculate evaluation metrics\n",
    "#     f1 = f1_score(Y, y_pred)\n",
    "#     precision = precision_score(Y, y_pred)\n",
    "#     recall = recall_score(Y, y_pred)\n",
    "#     tn, fp, fn, tp = confusion_matrix(Y, y_pred).ravel()\n",
    "#     specificity = tn / (tn + fp)\n",
    "    \n",
    "#     print(f\"Classifier: {type(classifier).__name__}\")\n",
    "#     print(f\"F1 score: {f1}\")\n",
    "#     print(f\"Precision: {precision}\")\n",
    "#     print(f\"Recall/Sensitivity: {recall}\")\n",
    "#     print(f\"Specificity: {specificity}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(confusion_matrix(Y, y_pred))\n",
    "#     print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Saving the models***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # In the path where you want to save the models\n",
    "# path = r'/Users/bogdancristianmihaila/Desktop/2nd Semester/Github/project2/Project-2-Medical-Imaging/Pickle/final_pickles'\n",
    "\n",
    "# # Save the models\n",
    "# #pickle.dump(LR, open(path + r'/LR_good.pkl', 'wb'))\n",
    "# pickle.dump(KNN, open(path + r'/KNN_bad.pkl', 'wb'))\n",
    "# #pickle.dump(DTC, open(path + r'\\DTC.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LR_good.joblib']"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(LR, 'LR_good.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
